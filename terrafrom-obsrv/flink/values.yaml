
namespace: "flink-local"
imagepullsecrets: ""
dockerhub: ""
repository: "divyagovindaiah123/sanketika-obsrv-flink-job"
image_tag: "1.0.0"
serviceMonitor:
  enabled: false
replicaCount: 1

jobmanager:
  rpc_port: 6123
  blob_port: 6124
  query_port: 6125
  ui_port: 8081
  prom_port: 9250
  heap_memory: 1024

rest_port: 80
resttcp_port: 8081
service:
  type: ClusterIP


taskmanager:
  prom_port: 9251
  rpc_port: 6122
  heap_memory: 1024
  replicas: 1
  cpu_requests: 0.3

checkpoint_store_type: ""

# AWS S3 Details
s3_access_key: ""
s3_secret_key: ""
s3_endpoint: ""
s3_path_style_access: ""


dataset_registry: |
  postgres {
    host = "192.168.1.38"
    port = 5432
    maxConnections = 2
    user = "obsrv"
    password = "obsrv123"
    database = "obsrv-registry"
  }
log4j_console_properties: |
  # This affects logging for both user code and Flink
  rootLogger.level = INFO
  rootLogger.appenderRef.console.ref = ConsoleAppender
  # Uncomment this if you want to _only_ change Flink's logging
  #logger.flink.name = org.apache.flink
  #logger.flink.level = INFO
  # The following lines keep the log level of common libraries/connectors on
  # log level INFO. The root logger does not override this. You have to manually
  # change the log levels here.
  logger.akka.name = akka
  logger.akka.level = ERROR
  logger.kafka.name= org.apache.kafka
  logger.kafka.level = ERROR
  logger.hadoop.name = org.apache.hadoop
  logger.hadoop.level = ERROR
  logger.zookeeper.name = org.apache.zookeeper
  logger.zookeeper.level = ERROR
  # Log all infos to the console
  appender.console.name = ConsoleAppender
  appender.console.type = CONSOLE
  appender.console.layout.type = PatternLayout
  appender.console.layout.pattern = %d{yyyy-MM-dd HH:mm:ss,SSS} %-5p %-60c %x - %m%n
  # Suppress the irrelevant (wrong) warnings from the Netty channel handler
  logger.netty.name = org.apache.flink.shaded.akka.org.jboss.netty.channel.DefaultChannelPipeline
  logger.netty.level = OFF
base_config: |
  kafka {
    consumer.broker-servers = "192.168.1.38:9092"
    producer {
      broker-servers = "192.168.1.38:9092"
      max-request-size = 1572864
      batch.size = 98304
      linger.ms = 10
      compression = "snappy"
  }
    output.system.event.topic = local.system.events
  }

  job {
    env = "local"
    enable.distributed.checkpointing = false
    statebackend {
      blob {
      storage {
        account = "blob.storage.account"
        container = "obsrv-container"
        checkpointing.dir = "flink-jobs"
      }
     }
     base.url = "wasbs://"${job.statebackend.blob.storage.container}"@"${job.statebackend.blob.storage.account}"/"${job.statebackend.blob.storage.checkpointing.dir}
  }
  }

  task {
    checkpointing.compressed = true
    checkpointing.interval = 60000
    checkpointing.pause.between.seconds = 30000
    restart-strategy.attempts = 3
    restart-strategy.delay = 30000 # in milli-seconds
    parallelism = 1
    consumer.parallelism = 1
   }

  # redis {
  #   host = localhost
  #   port = 6379
  #   connection.timeout = 30000
  #   database {
  #     default = 0
  #   }
  # }
  redis {
    host = "localhost"
    port = 6379
    connection.timeout = 30000
    database {
      extractor.duplication.store.id = 1
      preprocessor.duplication.store.id = 2
      key.expiry.seconds = 3600
    }
  }


  redis-meta {
    host = localhost
    port = 6379
    database {
      default = 0
  }
    
     
  }

  postgres {
    host = "192.168.1.38"
    port = 5432
    maxConnections = 2
    user = "postgres"
    password = "postgres"
    database = "obsrv"
  }

  lms-cassandra {
    host = "localhost"
    port = "9042"
  }
  
  
merged-pipeline:
  merged-pipeline: |+
    include file("/data/conf/base-config.conf")
    kafka {
      input.topic = local.ingest
      output.raw.topic = local.raw
      output.extractor.duplicate.topic = local.extractor.duplicate
      output.failed.topic = local.failed
      output.batch.failed.topic = local.extractor.failed
      event.max.size = "1048576" # Max is only 1MB
      output.invalid.topic = local.invalid
      output.unique.topic = local.unique
      output.duplicate.topic = local.duplicate
      output.denorm.topic = local.denorm
      output.denorm.failed.topic = local.denorm.failed
      output.transform.topic = local.transform
      stats.topic = local.stats
      groupId = local-single-pipeline-group
      producer {
        max-request-size = 5242880
      }
    }

    task {
      window.time.in.seconds = 5
      window.count = 30
      window.shards = 1400
      consumer.parallelism = 1
      downstream.operators.parallelism = 1
    }

    redis {
      database {
        extractor.duplication.store.id = 1
        preprocessor.duplication.store.id = 2
        key.expiry.seconds = 3600
      }
    }
    

    
  flink-conf: |+
    jobmanager.memory.flink.size: 1024m
    taskmanager.memory.flink.size: 1024m
    taskmanager.numberOfTaskSlots: 1
    jobManager.numberOfTaskSlots: 1
    parallelism.default: 1
    jobmanager.execution.failover-strategy: region
    taskmanager.memory.network.fraction: 0.1
    scheduler-mode: reactive
    heartbeat.timeout: 8000
    heartbeat.interval: 5000
    taskmanager.memory.process.size: 1700m
    jobmanager.memory.process.size: 1600m
    # classloader.resolve-order: "parent-first"
    # state.savepoints.dir: file:///tmp
  job_classname: org.sunbird.obsrv.pipeline.task.MergedPipelineStreamTask


transformer:
  transformer: |+
    include file("/data/conf/base-config.conf")
    include file("/data/flink/conf/dataset-registry.conf")

    kafka {
      input.topic = "local.denorm"
      output.transform.topic = "local.transform"
      groupId = "local-transformer-group"
      producer {
        max-request-size = 5242880
      }
    }

    task {
      consumer.parallelism = 1
      downstream.operators.parallelism = 1
    }
  
  flink-conf: |+
    jobmanager.memory.flink.size: 1024m
    jobmanager.heap.size: 512m
    taskmanager.memory.flink.size: 1024m
    taskmanager.numberOfTaskSlots: 1
    jobManager.numberOfTaskSlots: 1
    parallelism.default: 1
    jobmanager.execution.failover-strategy: region
    taskmanager.memory.network.fraction: 0.1
    scheduler-mode: reactive
    heartbeat.timeout: 8000
    heartbeat.interval: 5000
    taskmanager.memory.process.size: 1700m
    jobmanager.memory.process.size: 1600m
  job_classname: "org.sunbird.obsrv.transformer.task.TransformerStreamTas"


preprocessor: 
  preprocessor: |+
    include file("/data/conf/base-config.conf")
    kafka {
      input.topic = ${.raw"
      output.failed.topic = "local.failed"
      output.invalid.topic = "local.invalid"
      output.unique.topic = "local.unique"
      output.duplicate.topic = "local.duplicate"
      groupId = "local-pipeline-preprocessor-group"
    }

    task {
      consumer.parallelism = 1
      downstream.operators.parallelism = 1
    }

    redis {
      database {
        preprocessor.duplication.store.id = 2
        key.expiry.seconds = 3600
      }
    }
  flink-conf: |+
    jobmanager.memory.flink.size: 1024m
    taskmanager.memory.flink.size: 1024m
    taskmanager.numberOfTaskSlots: 1
    jobManager.numberOfTaskSlots: 1
    parallelism.default: 1
    jobmanager.execution.failover-strategy: region
    taskmanager.memory.network.fraction: 0.1
    scheduler-mode: reactive
    heartbeat.timeout: 8000
    heartbeat.interval: 5000
    taskmanager.memory.process.size: 1700m
    jobmanager.memory.process.size: 1600m 

  job_classname: "org.sunbird.obsrv.preprocessor.task.PipelinePreprocessorStreamTask"


extractor: 
  extractor: |+
    include file("/data/base-config.conf")
    kafka {
      input.topic = "local.ingest"
      output.raw.topic = "local.raw"
      output.extractor.duplicate.topic = "local.extractor.duplicate"
      output.failed.topic = "local.failed"
      output.batch.failed.topic = "local.extractor.failed"
      event.max.size = "1048576" # Max is only 1MB
      groupId = "local-extractor-group"
      producer {
        max-request-size = 5242880
      }
    }

    task {
      consumer.parallelism = 1
      downstream.operators.parallelism = 1
    }

    redis {
      database {
        extractor.duplication.store.id = 1
        key.expiry.seconds = 3600
      }
    }
  flink-conf: |+
    jobmanager.memory.flink.size: 1024m
    taskmanager.memory.flink.size: 1024m
    taskmanager.numberOfTaskSlots: 1
    jobManager.numberOfTaskSlots: 1
    parallelism.default: 1
    jobmanager.execution.failover-strategy: region
    taskmanager.memory.network.fraction: 0.1
    scheduler-mode: reactive
    heartbeat.timeout: 8000
    heartbeat.interval: 5000
    taskmanager.memory.process.size: 1700m
    jobmanager.memory.process.size: 1600m     
  job_classname: "org.sunbird.obsrv.extractor.task.ExtractorStreamTask"


druid-router:
  druid-router: |+
    include file("/data/conf/base-config.conf")
    kafka {
      input.topic = local".transform"
      stats.topic = local".stats"
      groupId = local"-druid-router-group"
    }

    task {
      consumer.parallelism = 1
      downstream.operators.parallelism = 1
    }
  flink-conf: |+
    jobmanager.memory.flink.size: 1024m
    taskmanager.memory.flink.size: 1024m
    taskmanager.numberOfTaskSlots: 1
    jobManager.numberOfTaskSlots: 1
    parallelism.default: 1
    jobmanager.execution.failover-strategy: region
    taskmanager.memory.network.fraction: 0.1
    scheduler-mode: reactive
    heartbeat.timeout: 8000
    heartbeat.interval: 5000
    taskmanager.memory.process.size: 1700m
    jobmanager.memory.process.size: 1600m   
  job_classname: "org.sunbird.obsrv.router.task.DruidRouterStreamTask"


denormalizer: 
  denormalizer: |+
    include file("/data/conf/base-config.conf")
    kafka {
      input.topic = "local.unique"
      output.denorm.topic = "local.denorm"
      output.denorm.failed.topic = "local.denorm.failed"
      groupId = "local-denormalizer-group"
    }

    task {
      window.time.in.seconds = 5
      window.count = 30
      window.shards = 1400
      consumer.parallelism = 1
      downstream.operators.parallelism = 1
    }
  flink-conf: |+
    jobmanager.memory.flink.size: 1024m
    taskmanager.memory.flink.size: 1024m
    taskmanager.numberOfTaskSlots: 1
    jobManager.numberOfTaskSlots: 1
    parallelism.default: 1
    jobmanager.execution.failover-strategy: region
    taskmanager.memory.network.fraction: 0.1
    scheduler-mode: reactive
    heartbeat.timeout: 8000
    heartbeat.interval: 5000
    taskmanager.memory.process.size: 1700m
    jobmanager.memory.process.size: 1600m     
  job_classname: "org.sunbird.obsrv.denormalizer.task.DenormalizerWindowStreamTask"
